{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREDICTION ALOGORITHM 1: NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network is a supervised learning algortihtm which means that we provide it with input data containing the independent variables and the output data that contain dependent variable.\n",
    "\n",
    "Independent variables are:\n",
    "- GRE Scores( out of 340)\n",
    "- TOEFL Scores (out of 120)\n",
    "- University Rating ( out of 5)\n",
    "- Statement of Purpose (out of 5)\n",
    "- Letter of Recommendation Strength (out of 5)\n",
    "- Undergraduate GPA (out of 10)\n",
    "- Research Experience ( either 0 or 1)\n",
    "\n",
    "\n",
    "Dependent varibale:\n",
    "- Chance of acceptance (feature that we would like to predict, 1 for accepted and 0 for rejected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GREScore</th>\n",
       "      <th>TOEFLScore</th>\n",
       "      <th>UniversityRating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>ChanceOfAdmit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>396</td>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GREScore  TOEFLScore  UniversityRating  SOP  LOR   CGPA  \\\n",
       "0             1       337         118                 4  4.5   4.5  9.65   \n",
       "1             2       324         107                 4  4.0   4.5  8.87   \n",
       "2             3       316         104                 3  3.0   3.5  8.00   \n",
       "3             4       322         110                 3  3.5   2.5  8.67   \n",
       "4             5       314         103                 2  2.0   3.0  8.21   \n",
       "..          ...       ...         ...               ...  ...   ...   ...   \n",
       "395         396       324         110                 3  3.5   3.5  9.04   \n",
       "396         397       325         107                 3  3.0   3.5  9.11   \n",
       "397         398       330         116                 4  5.0   4.5  9.45   \n",
       "398         399       312         103                 3  3.5   4.0  8.78   \n",
       "399         400       333         117                 4  5.0   4.0  9.66   \n",
       "\n",
       "     Research  ChanceOfAdmit  \n",
       "0           1           0.92  \n",
       "1           1           0.76  \n",
       "2           1           0.72  \n",
       "3           1           0.80  \n",
       "4           0           0.65  \n",
       "..        ...            ...  \n",
       "395         1           0.82  \n",
       "396         1           0.84  \n",
       "397         1           0.91  \n",
       "398         0           0.67  \n",
       "399         1           0.95  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = 'Admission_Predict.csv' \n",
    "df = pandas.read_csv(data_file) #reading our data file\n",
    "\n",
    "df #our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANALYSING AND PROCESSING OUR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.  , 337.  , 118.  , ...,   9.65,   1.  ,   0.92],\n",
       "       [  2.  , 324.  , 107.  , ...,   8.87,   1.  ,   0.76],\n",
       "       [  3.  , 316.  , 104.  , ...,   8.  ,   1.  ,   0.72],\n",
       "       ...,\n",
       "       [398.  , 330.  , 116.  , ...,   9.45,   1.  ,   0.91],\n",
       "       [399.  , 312.  , 103.  , ...,   8.78,   0.  ,   0.67],\n",
       "       [400.  , 333.  , 117.  , ...,   9.66,   1.  ,   0.95]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = df.values #converting our dataframe into an array\n",
    "dataset#our array of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature data: \n",
      " [[337.   118.     4.   ...   4.5    9.65   1.  ]\n",
      " [324.   107.     4.   ...   4.5    8.87   1.  ]\n",
      " [316.   104.     3.   ...   3.5    8.     1.  ]\n",
      " ...\n",
      " [330.   116.     4.   ...   4.5    9.45   1.  ]\n",
      " [312.   103.     3.   ...   4.     8.78   0.  ]\n",
      " [333.   117.     4.   ...   4.     9.66   1.  ]]\n",
      "labelled data: \n",
      " [0.92 0.76 0.72 0.8  0.65 0.9  0.75 0.68 0.5  0.45 0.52 0.84 0.78 0.62\n",
      " 0.61 0.54 0.66 0.65 0.63 0.62 0.64 0.7  0.94 0.95 0.97 0.94 0.76 0.44\n",
      " 0.46 0.54 0.65 0.74 0.91 0.9  0.94 0.88 0.64 0.58 0.52 0.48 0.46 0.49\n",
      " 0.53 0.87 0.91 0.88 0.86 0.89 0.82 0.78 0.76 0.56 0.78 0.72 0.7  0.64\n",
      " 0.64 0.46 0.36 0.42 0.48 0.47 0.54 0.56 0.52 0.55 0.61 0.57 0.68 0.78\n",
      " 0.94 0.96 0.93 0.84 0.74 0.72 0.74 0.64 0.44 0.46 0.5  0.96 0.92 0.92\n",
      " 0.94 0.76 0.72 0.66 0.64 0.74 0.64 0.38 0.34 0.44 0.36 0.42 0.48 0.86\n",
      " 0.9  0.79 0.71 0.64 0.62 0.57 0.74 0.69 0.87 0.91 0.93 0.68 0.61 0.69\n",
      " 0.62 0.72 0.59 0.66 0.56 0.45 0.47 0.71 0.94 0.94 0.57 0.61 0.57 0.64\n",
      " 0.85 0.78 0.84 0.92 0.96 0.77 0.71 0.79 0.89 0.82 0.76 0.71 0.8  0.78\n",
      " 0.84 0.9  0.92 0.97 0.8  0.81 0.75 0.83 0.96 0.79 0.93 0.94 0.86 0.79\n",
      " 0.8  0.77 0.7  0.65 0.61 0.52 0.57 0.53 0.67 0.68 0.81 0.78 0.65 0.64\n",
      " 0.64 0.65 0.68 0.89 0.86 0.89 0.87 0.85 0.9  0.82 0.72 0.73 0.71 0.71\n",
      " 0.68 0.75 0.72 0.89 0.84 0.93 0.93 0.88 0.9  0.87 0.86 0.94 0.77 0.78\n",
      " 0.73 0.73 0.7  0.72 0.73 0.72 0.97 0.97 0.69 0.57 0.63 0.66 0.64 0.68\n",
      " 0.79 0.82 0.95 0.96 0.94 0.93 0.91 0.85 0.84 0.74 0.76 0.75 0.76 0.71\n",
      " 0.67 0.61 0.63 0.64 0.71 0.82 0.73 0.74 0.69 0.64 0.91 0.88 0.85 0.86\n",
      " 0.7  0.59 0.6  0.65 0.7  0.76 0.63 0.81 0.72 0.71 0.8  0.77 0.74 0.7\n",
      " 0.71 0.93 0.85 0.79 0.76 0.78 0.77 0.9  0.87 0.71 0.7  0.7  0.75 0.71\n",
      " 0.72 0.73 0.83 0.77 0.72 0.54 0.49 0.52 0.58 0.78 0.89 0.7  0.66 0.67\n",
      " 0.68 0.8  0.81 0.8  0.94 0.93 0.92 0.89 0.82 0.79 0.58 0.56 0.56 0.64\n",
      " 0.61 0.68 0.76 0.86 0.9  0.71 0.62 0.66 0.65 0.73 0.62 0.74 0.79 0.8\n",
      " 0.69 0.7  0.76 0.84 0.78 0.67 0.66 0.65 0.54 0.58 0.79 0.8  0.75 0.73\n",
      " 0.72 0.62 0.67 0.81 0.63 0.69 0.8  0.43 0.8  0.73 0.75 0.71 0.73 0.83\n",
      " 0.72 0.94 0.81 0.81 0.75 0.79 0.58 0.59 0.47 0.49 0.47 0.42 0.57 0.62\n",
      " 0.74 0.73 0.64 0.63 0.59 0.73 0.79 0.68 0.7  0.81 0.85 0.93 0.91 0.69\n",
      " 0.77 0.86 0.74 0.57 0.51 0.67 0.72 0.89 0.95 0.79 0.39 0.38 0.34 0.47\n",
      " 0.56 0.71 0.78 0.73 0.82 0.62 0.96 0.96 0.46 0.53 0.49 0.76 0.64 0.71\n",
      " 0.84 0.77 0.89 0.82 0.84 0.91 0.67 0.95]\n"
     ]
    }
   ],
   "source": [
    "#Here we split our dataset into input features X and the feature we wish to predict\n",
    "\n",
    "X = dataset[:,1:8] #here we remove the first column and take the remaining dataset with the exception of the feture we want to predict\n",
    "Y_ = dataset[:,8] #get our feature that we need to predict\n",
    "\n",
    "print(\"feature data: \\n\", X)\n",
    "print(\"labelled data: \\n\",Y_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONVERTING ACCEPTANCE CHANCE TO CLASSIFICATION THROUGH ROUNDING UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = [] #list to store new rounded up acceptance chance column values\n",
    "threshold = 0.5 # our threshold for rounding\n",
    "\n",
    "#convert values to 0 = not accepted and 1 = accepted\n",
    "\n",
    "for i in Y_: #loop through label data \n",
    "    if i < threshold:\n",
    "        i = 0 #set to 0 \n",
    "        Y.append(i) #store updated values\n",
    "    \n",
    "    else:\n",
    "        i = 1  #otherwise set to 1\n",
    "        Y.append(i) #store updated values\n",
    "Y = np.array(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X scaled: \n",
      " [[0.94       0.92857143 0.75       ... 0.875      0.91346154 1.        ]\n",
      " [0.68       0.53571429 0.75       ... 0.875      0.66346154 1.        ]\n",
      " [0.52       0.42857143 0.5        ... 0.625      0.38461538 1.        ]\n",
      " ...\n",
      " [0.8        0.85714286 0.75       ... 0.875      0.84935897 1.        ]\n",
      " [0.44       0.39285714 0.5        ... 0.75       0.63461538 0.        ]\n",
      " [0.86       0.89285714 0.75       ... 0.75       0.91666667 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#here we process our data to make sure that the scale of the input features are similar\n",
    "#we do this to avoid difficulties for the initialization of the neutral network\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X) #Scaling the dataset so that all input features lie between 0 and 1 inclusive\n",
    "print(\"X scaled: \\n\",X_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIVIDING THE DATA\n",
    "\n",
    "7 features\n",
    "\n",
    "- X-training data = 204 (60%)\n",
    "- X- validation data = 68 (20%)\n",
    "- X- testing data = 68 (20%)\n",
    "\n",
    "Chance of admission feature\n",
    "\n",
    "- Y-training data = 204 (60%)\n",
    "- Y- validation data = 68 (20%)\n",
    "- Y- testing data = 68 (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Shape of training data:  (240, 7)\n",
      "Shape of training label:  (240,)\n",
      "Shape of validation data:  (80, 7)\n",
      "Shape of validation label:  (80,)\n",
      "Shape of testing data:  (80, 7)\n",
      "Shape of testing label:  (80,)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# validation and testing takes 40% of the original data and training takes 60% and randomizing\n",
    "X_training,X_validation_test, Y_training_label, Y_validation_test =  train_test_split(X_scale, Y, test_size = 0.4,random_state = 2) # here we split trainig data and validation/testing data into two\n",
    "\n",
    "#spliting validation and testing and randomizing\n",
    "\n",
    "X_validation, X_testing, Y_validation_label, Y_testing_label = train_test_split(X_validation_test,Y_validation_test,test_size = 0.5,random_state = 2)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Shape of training data: \",X_training.shape)\n",
    "print(\"Shape of training label: \",Y_training_label.shape)\n",
    "print(\"Shape of validation data: \",X_validation.shape)\n",
    "print(\"Shape of validation label: \",Y_validation_label.shape)\n",
    "print(\"Shape of testing data: \",X_testing.shape)\n",
    "print(\"Shape of testing label: \",Y_testing_label.shape)\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240,)\n",
      "(240, 1)\n",
      "()\n",
      "(240, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (240,240) and (8,1) not aligned: 240 (dim 1) != 8 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e8a6374baecd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[0mNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_training\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_training_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-e8a6374baecd>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-e8a6374baecd>\u001b[0m in \u001b[0;36mback_prop\u001b[1;34m(self, yhat)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mdl_wrt_A1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdl_wrt_z2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[0mdl_wrt_w2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_wrt_z2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mdl_wrt_b2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdl_wrt_z2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (240,240) and (8,1) not aligned: 240 (dim 1) != 8 (dim 0)"
     ]
    }
   ],
   "source": [
    "class NeuralNet():\n",
    "        #class takes in list that stores our network architecture, learning rate and iterations\n",
    "        # layers[7 input features, 8 nodes for hidden layer, 1 output]\n",
    "        \n",
    "        def __init__(self, layers = [7,8,1], learning_rate = 0.001, iterations = 100):\n",
    "            self.params = {} #dictionary to store weights\n",
    "            self.learning_rate = learning_rate\n",
    "            self.iterations = iterations\n",
    "            self.loss = []\n",
    "            self.sample_size  = None\n",
    "            self.layers = layers \n",
    "            self.X = None\n",
    "            self.Y = None\n",
    "        \n",
    "        #initializes weights for the network\n",
    "        #we use unifrom distribution for our weights and store them in the params dictionary \n",
    "        def init_weights(self):\n",
    "            \n",
    "            np.random.seed(1) #seed that random number of generator\n",
    "            self.params['W1'] = np.random.randn(self.layers[0], self.layers[1]) #array of shape(7,8)\n",
    "            self.params['b1'] = np.random.randn(self.layers[1],) #vector of size 8\n",
    "            self.params['W2'] = np.random.randn(self.layers[1], self.layers[2]) #array of shape(8,1)\n",
    "            self.params['b2'] = np.random.randn(self.layers[2],)#vector of size 1, the output\n",
    "            \n",
    "        #for our network we will use an activation function (ReLU function) for hidden layer\n",
    "        #This function compares a value with zero then return the value passed to if if it is greater that zero\n",
    "        #It will perform threshold operation to each value input\n",
    "        \n",
    "        def ReLU(self,Z):\n",
    "            return np.maximum(0,Z)\n",
    "        \n",
    "        #we will also use sigmoid function for our ouput layer\n",
    "        #takes in input value and squashes it to a real valued output between 0 and 1\n",
    "        \n",
    "        def sigmoid(self, Z):\n",
    "            return 1.0/(1.0 + np.exp(-Z))\n",
    "        \n",
    "        #here we gonna have a function for loss function\n",
    "        # it measures how good our neural network is predicting so that it can adjust the weights\n",
    "        \n",
    "        def entropy_loss(self,y, y_hat):\n",
    "            nsample = len(y)\n",
    "            loss = -1/nsample * (np.sum(np.multiply(np.log(y_hat),y) + np.multiply((1 - y), np.log(1 - y_hat))))\n",
    "            return loss\n",
    "        \n",
    "        \n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
